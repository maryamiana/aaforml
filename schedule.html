<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Applied Algorithms for ML</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
         <style>
        .abstract {
            display: none; /* Initially hide all abstracts */
        }
        </style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Applied Algorithms for ML </strong> </a>
									<ul class="icons">
										<li><a href="https://twitter.com/RiceUniversity" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<!--
                                        <li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-snapchat-ghost"><span class="label">Snapchat</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-medium-m"><span class="label">Medium</span></a></li>
                                        -->
									</ul>
								</header>

								<section>
									<header class="main">
										<h1>Schedule</h1>
									</header>
                                    
                                    

                                    
                          
<div>
        <h2>Day 1: Monday, June 10, 2024</h2>
        <table class="table">
            
            <colgroup>
            <col style="width: 25%;"> 
            <col style="width: 45%;"> 
            <col style="width: 30%;"> 
            </colgroup>
            
            <tr style="font-size: 1.2em; font-weight: bold;"><th>Time</th><th>Event</th><th>Location</th></tr>
            <tr><td>8:00 - 9:00 AM</td><td>Breakfast</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>8:00 - 9:15 AM</td><td>Opening Remarks</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>9:15 - 10:00 AM</td><td>
                <a href="#" class="toggle-abstract">Adversarial Training Should Be Cast as a Non-Zero-Sum Game</a>
                <div class="abstract-content">One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation naturally yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial training algorithms, and does not suffer from robust overfitting.
                </div>
                <br>
                By <a href="https://people.epfl.ch/volkan.cevher?lang=en">Volkan Cevher (EPFL)</a>
            </td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>10:00 - 10:45 AM</td><td><a href="#" class="toggle-abstract">The Combinatorial Side of Graph Neural Networks</a>
                <div class="abstract-content">
Graph Neural Networks (GNNs) are a machine learning architecture to learn functions on graphs. For example, since problem instances for combinatorial optimisation tasks are often modelled as graphs, GNNs have recently received attention as a natural framework for finding good heuristics in neural optimisation approaches. 
The question which functions can actually be learnt by message-passing GNNs and which ones exceed their power has been studied extensively. In this talk, I will consider it from a graph-theoretical perspective. I will survey the Weisfeiler-Leman algorithm as a combinatorial procedure to analyse and compare graph structure, and I will discuss some results concerning the power of the algorithm on natural graph classes. The findings directly translate into insights about the power of GNNs and of their extensions to higher-dimensional neural networks.
                </div>
                <br>
                By 
<a href="https://www.jesus.ox.ac.uk/about-jesus-college/our-community/people/dr-sandra-kiefer/">Sandra Kiefer (Oxford)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>10:45 - 11:15 AM</td><td>Coffee Break</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>11:15 AM - 12:00 PM</td><td><a href="https://www.mit.edu/~parrilo/">Pablo Parrilo (MIT)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>12:00 - 2:00 PM</td><td>Lunch (on your own) <br> <a href="Restaurant_Recommendations.pdf" target="_blank">Restaurant recommendations</a></td><td></td></tr>
            <tr><td>2:00 - 2:45 PM</td><td><a href="#" class="toggle-abstract">Self-supervised learning for the geosciences</a>
                <div class="abstract-content">
Digitized data has become abundant, especially in the geosciences, but preprocessing it to create the “labeled data” needed for supervised machine learning is often costly, time-consuming, or even impossible. Fortuitously, in very large-scale data domains, “self-supervised” machine learning methods are now actually outperforming supervised learning methods. In this talk, I will first define self-supervised deep learning, including the notion of a “pretext task.” Then I will survey our lab’s work developing self-supervised learning approaches for several tasks in the geosciences, such as downscaling and temporal interpolation of spatiotemporal data.
                </div>
                <br>
                By 
<a href="https://www.colorado.edu/faculty/claire-monteleoni/">Claire Monteleoni (INRIA/Boulder)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>2:45 - 3:30 PM</td><td><a href="#" class="toggle-abstract">Divide-and-Conquer Posterior Sampling with Denoising Diffusion Priors</a>
                <div class="abstract-content">
			Recent advancements in Bayesian inverse problem-solving have spotlighted Denoising Diffusion Models (DDMs) as effective priors. Despite their potential, the challenge lies in efficient sampling from the complex posterior distribution these models yield. Classical approaches necessitate either cumbersome retraining of model-specific components to align with the likelihood or introduce approximation errors affecting the accuracy of the posterior. In our study, we introduce an innovative framework that leverages the inherent structure of DDMs to construct a series of simplified diffusion guidance problems. This novel method significantly reduces the approximation error associated with current techniques, without the need for retraining. We demonstrate the versatility and effectiveness of our approach across a broad spectrum of Bayesian inverse problems.
                </div>
                <br>
                By <a href="https://scholar.google.fr/citations?user=_XE1LvQAAAAJ&hl=en">Éric Moulines (Ecole Polytechnique)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>3:30 - 4:00 PM</td><td>Coffee Break</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>4:00 - 4:45 PM</td><td><a href="#" class="toggle-abstract">Probabilistic Embedding in MPC  and Strong OV conjecture </a>
                <div class="abstract-content">
                </div>
                <br>
                By <a href="https://www.cs.umd.edu/~hajiagha/">MohammadTaghi HajiAghayi (UMD)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>4:45 - 5:30 PM</td><td><a href="#" class="toggle-abstract">Self-Play Preference Optimization for Language Model Alignment</a>
                <div class="abstract-content">
			Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-play Probabilistic Preference Optimization (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys a theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.
                </div>
                <br>
                By <a href="https://web.cs.ucla.edu/~qgu/">Quanquan Gu (UCLA)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>5:30 - 7:00 PM</td><td>Poster Session + Reception <br><span style="color: red;">(Note: Change of location)</span>  </td><td><a href="https://maps.app.goo.gl/LtCNhrAHx3SgRfwy5">Rice Global Paris Center</a></td></tr>
            
            
        
        </table>

        <h2>Day 2: Tuesday, June 11, 2024</h2>
        <table class="table">
            
            <colgroup>
            <col style="width: 25%;"> 
            <col style="width: 45%;"> 
            <col style="width: 30%;"> 
            </colgroup>
            
            <tr style="font-size: 1.2em; font-weight: bold;"><th>Time</th><th>Event</th><th>Location</th></tr>
            <tr><td>8:00 - 9:00 AM</td><td>Breakfast</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>9:00 - 9:45 AM</td><td><a href="#" class="toggle-abstract">A Bi-metric Framework for Fast Similarity Search</a>
                <div class="abstract-content">We propose a new ``bi-metric'' framework for designing nearest neighbor data structures. Our framework assumes two dissimilarity functions: a *ground-truth* metric that is accurate but expensive to compute, and a *proxy* metric that is cheaper but less accurate. In both theory and practice, we show how to construct data structures using only the proxy metric such that the query procedure achieves the accuracy of the expensive metric, while using only a limited number of calls to both metrics. Our theoretical results instantiate this framework for two popular nearest neighbor search algorithms: DiskANN and Cover Tree. In both cases we show that, as long as the proxy metric used to construct the data structure approximates the ground-truth metric up to a constant factor, our data structure achieves arbitrarily good approximation guarantees with respect to the ground-truth metric. On the empirical side, we apply the framework to the text retrieval problem with two dissimilarity functions evaluated by ML models with vastly different computational costs. We observe that for almost all data sets in the MTEB benchmark, our approach achieves a considerably better accuracy-efficiency tradeoff than the alternatives.
                </div>
                <br>
                By 

<a href="https://people.csail.mit.edu/indyk/">Piotr Indyk (MIT)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>9:45 - 10:30 AM</td><td><a href="https://www.dcs.warwick.ac.uk/~czumaj/">Artur Czumaj (U of Warwick)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>10:30 - 11:15 AM</td><td>Coffee Break</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>11:15 - 12:00 PM</td><td><a href="https://causalai.net/">Elias Bareinboim (Columbia)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>12:00 - 2:00 PM</td><td>Lunch (on your own) <br> <a href="Restaurant_Recommendations.pdf" target="_blank">Restaurant recommendations</a></td><td></td></tr>
            <tr><td>2:00 - 2:45 PM</td><td><a href="https://people.cs.georgetown.edu/~kobbi/">Kobbi Nissim (Georgetown)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>2:45 - 3:30 PM</td><td><a href="https://people.eecs.berkeley.edu/~minilek/">Jelani Nelson (UC Berkeley)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>3:30 - 4:00 PM</td><td>Coffee Break</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>4:00 - 4:45 PM</td><td><a href="https://richtarik.org">Peter Richtarik (KAUST)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>4:45 - 5:30 PM</td><td><a href="https://www.fawzi.ai/">Alhussein Fawzi (DeepMind)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>5:30 - 7:00 PM</td><td>Poster Session + Light Reception <br><span style="color: red;">(Note: Change of location)</span>  </td><td><a href="https://maps.app.goo.gl/LtCNhrAHx3SgRfwy5">Rice Global Paris Center</a></td></tr>
        </table>

        <h2>Day 3: Wednesday, June 12, 2024</h2>
        <table class="table">
            
            <colgroup>
            <col style="width: 25%;"> 
            <col style="width: 45%;"> 
            <col style="width: 30%;"> 
            </colgroup>
            
            <tr style="font-size: 1.2em; font-weight: bold; "><th>Time</th><th>Event</th><th>Location</th></tr>
            <tr><td>8:00 - 9:00 AM</td><td>Breakfast</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>9:00 - 9:45 AM</td><td>
<a href="#" class="toggle-abstract">Hash functions bridging the gap from theory to practice</a>
                <div class="abstract-content">Randomized algorithms are often enjoyed for their simplicity, but the
hash functions employed to yield the desired probabilistic guarantees
are often too complicated to be practical.

Hash functions are used everywhere in computing, e.g., hash tables,
sketching, dimensionality reduction, sampling, and estimation. Many of
these applications are relevant to Machine Learning, where we are
often interested in similarity between high dimensional objects. Reducing
the dimensionality is key to efficient processing.

Abstractly, we like to think of hashing as fully-random
hashing, assigning independent hash values to every possible key, but
essentially this requires us to store the hash values for all keys,
which is unrealistic for most key universes, e.g., 64-bit keys.

In practice we have to settle for implementable hash functions, and
often practitioners settle for implementations that are too simple in
that the algorithms ends up working only for sufficiently random
input. However, the real world is full of structured/non-random input.

The issue is severe, for simplistic hash functions will often work very well
in tests with random input. Moreover, the issue is often that error
events that should never happen in practice, happen with way too high
probability. This does not show in a few test, but will show up over
time when you put the system into production.

Over the last decade there has been major developments in simple to
implement tabulation based hash functions offering strong theoretical
guarantees, so as to support fundamental properties such as Chernoff
bounds, Sparse Johnson-Lindenstrauss transforms, and fully-random
hashing on a given set w.h.p. etc.

I will discuss some of the principles of these developments and offer
insights on how far we can bridge from theory (assuming fully-random
hash functions) to practice (needing something that can actually
implemented efficiently).
                </div>
                <br>
                By <a href="http://hjemmesider.diku.dk/~mthorup/">Mikkel Thorup (U of Copenhagen)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>9:45 - 10:30 AM</td><td><a href="#" class="toggle-abstract">An alternative view of denoising diffusion models</a>
                <div class="abstract-content"> Denoising diffusion models have led to impressive generative models in many domains. In this talk, I will present recent progress, with a focus on formulations that do not involve stochastic differential equations.
                </div>
                <br>
                By <a href="https://www.di.ens.fr/~fbach/">Francis Bach (INRIA)</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>10:30 - 11:00 AM</td><td>Coffee Break</td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>11:00 - 11:20 AM</td><td><b>Rising Stars talk:</b> <a href="http://drlinyang.net">Lin Yang</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
            <tr><td>11:20 - 11:40 AM</td><td><b>Rising Stars talk:</b> <a href="https://amartya18x.github.io">Amartya Sanyal</a></td><td><a href="https://maps.app.goo.gl/3q3VaUqAuZYRvpb37">Club de la Chasse</a></td></tr>
        </table>
    </div>


                                    
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
                            <!--
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>
                            -->

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Home</a></li>
                                        <li><a href="schedule.html">Schedule</a></li>
										<li><a href="travel.html">Travel Information</a></li>
										<!--
                                            <li>
											<span class="opener">Submenu</span>
											<ul>
												<li><a href="#">Lorem Dolor</a></li>
												<li><a href="#">Ipsum Adipiscing</a></li>
												<li><a href="#">Tempus Magna</a></li>
												<li><a href="#">Feugiat Veroeros</a></li>
											</ul>
										</li>
										<li><a href="#">Etiam Dolore</a></li>
										<li><a href="#">Adipiscing</a></li>
										<li>
											<span class="opener">Another Submenu</span>
											<ul>
												<li><a href="#">Lorem Dolor</a></li>
												<li><a href="#">Ipsum Adipiscing</a></li>
												<li><a href="#">Tempus Magna</a></li>
												<li><a href="#">Feugiat Veroeros</a></li>
											</ul>
										</li>
										<li><a href="#">Maximus Erat</a></li>
										<li><a href="#">Sapien Mauris</a></li>
										<li><a href="#">Amet Lacinia</a></li>
                                        -->
									</ul>
								</nav>

							<!-- Section -->
                            <!--
								<section>
									<header class="major">
										<h2>Ante interdum</h2>
									</header>
									<div class="mini-posts">
										<article>
											<a href="#" class="image"><img src="images/pic07.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic08.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic09.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
									</div>
									<ul class="actions">
										<li><a href="#" class="button">More</a></li>
									</ul>
								</section>
                            -->
                            

							<!-- Section -->
                            <!--
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<p>Sed varius enim lorem ullamcorper dolore aliquam aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin sed aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">information@untitled.tld</a></li>
										<li class="icon solid fa-phone">(000) 000-0000</li>
										<li class="icon solid fa-home">1234 Somewhere Road #8254<br />
										Nashville, TN 00000-0000</li>
									</ul>
								</section>
                                -->
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy;Rice University. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>
        
        

		<!-- Scripts -->
<script>
        document.addEventListener('DOMContentLoaded', function() {
            document.querySelectorAll('.toggle-abstract').forEach(toggle => {
                toggle.addEventListener('click', function(event) {
                    event.preventDefault();
                    const abstractContent = this.nextElementSibling;
                    abstractContent.classList.toggle('expanded');
                });
            });
        });
    </script>

			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>